{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "3Nt9zoAldMkk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/kunjee17/mahabharata/master/books/14.txt"
      ],
      "metadata": {
        "id": "MIwW_nQMJ-td",
        "outputId": "7ed0a559-5e6d-4a8f-b16d-8e19f98aad61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-19 10:51:50--  https://raw.githubusercontent.com/kunjee17/mahabharata/master/books/14.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 529736 (517K) [text/plain]\n",
            "Saving to: ‘14.txt.1’\n",
            "\n",
            "\r14.txt.1              0%[                    ]       0  --.-KB/s               \r14.txt.1            100%[===================>] 517.32K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-08-19 10:51:50 (84.5 MB/s) - ‘14.txt.1’ saved [529736/529736]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file=open(\"14.txt\",'r')\n",
        "text=file.read()\n"
      ],
      "metadata": {
        "id": "oAILi5HaMUYu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "id": "PwOzMHVnMeUt",
        "outputId": "186e8e8b-37a9-44c1-f6c4-6b353d480c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Mahabharata\\n\\nof\\n\\nKrishna-Dwaipayana Vyasa\\n\\nBOOK 14\\n\\nASWAMEDHA PARVA\\n\\nTranslated into English Prose from the Original Sanskrit Text\\n\\nby\\n\\nKisari Mohan Ganguli\\n\\n[1883-1896]\\n\\nScanned at sacred-texts.com, January 2004. Proofed by John Bruno Hare.\\n\\n\\n\\nTHE MAHABHARATA\\n\\nASWAMEDHA PARVA\\n\\nSECTION I\\n\\n(Aswamedhika Parva)\\n\\nOM! HAVING BOWED down unto Narayana, and Nara the foremost of male\\nbeings, and unto the goddess Saraswati, must the word Jaya be uttered.\\n\\n\"Vaisampayana said, \\'After the king Dhritarashtra had offered libations\\nof water (unto the manes of Bhisma), the mighty-armed[1] Yudhishthira,\\nwith his senses bewildered, placing the former in his front, ascended the\\nbanks (of the river), his eyes suffused with tears, and dropt down on the\\nbank of the Ganga like an elephant pierced by the hunter. Then incited by\\nKrishna, Bhima took him up sinking. \"This must not be so,\" said Krishna,\\nthe grinder of hostile hosts. The Pandavas, O king, saw Yudhishthira, the\\nson of Dharma, troubled and lying '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars=tuple(set(text))\n",
        "int2char=dict(enumerate(chars))\n",
        "char2int={ch:ii for ii,ch in int2char.items()}\n",
        "encoded=np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "wxUNnqVFMf0A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V29OqF5VNIBa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "QcGQyTMZNJMp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "metadata": {
        "id": "xndSKNuMNQAx",
        "outputId": "e9c1b35e-b2e1-44f1-f9df-201f14643acf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "\n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "\n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "blMqBJrTNqRg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "metadata": {
        "id": "am7WtkZLPY9k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "metadata": {
        "id": "PW0vq8UWPa9q",
        "outputId": "ba0c0bc2-0121-4cf1-bdf0-6db60c450d3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[15  4 41 70 62 31  4 31 59  4]\n",
            " [34 31 13 70 48 43 70 32 73 42]\n",
            " [10 43  4 41 27 50 31 20 12 31]\n",
            " [50 13  8 51 49  2 16 70 10 21]\n",
            " [70 21 27 13 70 52 12 70 27  4]\n",
            " [65 21 41 12 27 70 63 36 73 42]\n",
            " [ 4 21 43  4 58 31 70 27  4 41]\n",
            " [12 41 42 70 30 41 31  9 41 12]]\n",
            "\n",
            "y\n",
            " [[ 4 41 70 62 31  4 31 59  4 31]\n",
            " [31 13 70 48 43 70 32 73 42  4]\n",
            " [43  4 41 27 50 31 20 12 31 13]\n",
            " [13  8 51 49  2 16 70 10 21 12]\n",
            " [21 27 13 70 52 12 70 27  4 36]\n",
            " [21 41 12 27 70 63 36 73 42 12]\n",
            " [21 43  4 58 31 70 27  4 41 12]\n",
            " [41 42 70 30 41 31  9 41 12 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "metadata": {
        "id": "0J8Wo4zqPhWR",
        "outputId": "252e30ec-f316-4236-b7e0-ec230d33e851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "DhhoaDkbPmIx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    '''\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "yioyV2FKPsHn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "sQvTHl_qPvtE",
        "outputId": "4acb6bd9-4bfc-431b-9e1f-a5169598e535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(76, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=76, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "metadata": {
        "id": "RijeyedIPyUB",
        "outputId": "12ec2754-a440-4ff5-fac3-9d337424833e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2578... Val Loss: 3.2352\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1588... Val Loss: 3.1723\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1630... Val Loss: 3.1725\n",
            "Epoch: 2/20... Step: 40... Loss: 3.1694... Val Loss: 3.1609\n",
            "Epoch: 2/20... Step: 50... Loss: 3.1561... Val Loss: 3.1601\n",
            "Epoch: 2/20... Step: 60... Loss: 3.1262... Val Loss: 3.1592\n",
            "Epoch: 2/20... Step: 70... Loss: 3.1206... Val Loss: 3.1481\n",
            "Epoch: 3/20... Step: 80... Loss: 3.1234... Val Loss: 3.1329\n",
            "Epoch: 3/20... Step: 90... Loss: 3.0747... Val Loss: 3.0953\n",
            "Epoch: 3/20... Step: 100... Loss: 2.9977... Val Loss: 3.0348\n",
            "Epoch: 3/20... Step: 110... Loss: 2.9037... Val Loss: 2.9318\n",
            "Epoch: 4/20... Step: 120... Loss: 2.9337... Val Loss: 2.8751\n",
            "Epoch: 4/20... Step: 130... Loss: 2.7334... Val Loss: 2.7582\n",
            "Epoch: 4/20... Step: 140... Loss: 2.6198... Val Loss: 2.6729\n",
            "Epoch: 5/20... Step: 150... Loss: 2.5941... Val Loss: 2.6198\n",
            "Epoch: 5/20... Step: 160... Loss: 2.5278... Val Loss: 2.5950\n",
            "Epoch: 5/20... Step: 170... Loss: 2.4731... Val Loss: 2.5585\n",
            "Epoch: 5/20... Step: 180... Loss: 2.4439... Val Loss: 2.5314\n",
            "Epoch: 6/20... Step: 190... Loss: 2.4231... Val Loss: 2.5122\n",
            "Epoch: 6/20... Step: 200... Loss: 2.3540... Val Loss: 2.4790\n",
            "Epoch: 6/20... Step: 210... Loss: 2.3437... Val Loss: 2.4592\n",
            "Epoch: 6/20... Step: 220... Loss: 2.3085... Val Loss: 2.4307\n",
            "Epoch: 7/20... Step: 230... Loss: 2.2704... Val Loss: 2.4079\n",
            "Epoch: 7/20... Step: 240... Loss: 2.2439... Val Loss: 2.3906\n",
            "Epoch: 7/20... Step: 250... Loss: 2.2192... Val Loss: 2.3685\n",
            "Epoch: 8/20... Step: 260... Loss: 2.2603... Val Loss: 2.3754\n",
            "Epoch: 8/20... Step: 270... Loss: 2.1792... Val Loss: 2.3264\n",
            "Epoch: 8/20... Step: 280... Loss: 2.1480... Val Loss: 2.3170\n",
            "Epoch: 8/20... Step: 290... Loss: 2.1443... Val Loss: 2.2937\n",
            "Epoch: 9/20... Step: 300... Loss: 2.1212... Val Loss: 2.2723\n",
            "Epoch: 9/20... Step: 310... Loss: 2.0833... Val Loss: 2.2516\n",
            "Epoch: 9/20... Step: 320... Loss: 2.0684... Val Loss: 2.2408\n",
            "Epoch: 9/20... Step: 330... Loss: 2.0596... Val Loss: 2.2311\n",
            "Epoch: 10/20... Step: 340... Loss: 1.9775... Val Loss: 2.2111\n",
            "Epoch: 10/20... Step: 350... Loss: 2.0098... Val Loss: 2.2032\n",
            "Epoch: 10/20... Step: 360... Loss: 1.9678... Val Loss: 2.1873\n",
            "Epoch: 10/20... Step: 370... Loss: 1.9949... Val Loss: 2.1764\n",
            "Epoch: 11/20... Step: 380... Loss: 1.9293... Val Loss: 2.1569\n",
            "Epoch: 11/20... Step: 390... Loss: 1.9226... Val Loss: 2.1442\n",
            "Epoch: 11/20... Step: 400... Loss: 1.9047... Val Loss: 2.1372\n",
            "Epoch: 12/20... Step: 410... Loss: 1.9157... Val Loss: 2.1239\n",
            "Epoch: 12/20... Step: 420... Loss: 1.8766... Val Loss: 2.1173\n",
            "Epoch: 12/20... Step: 430... Loss: 1.8758... Val Loss: 2.1063\n",
            "Epoch: 12/20... Step: 440... Loss: 1.8698... Val Loss: 2.0898\n",
            "Epoch: 13/20... Step: 450... Loss: 1.8084... Val Loss: 2.0748\n",
            "Epoch: 13/20... Step: 460... Loss: 1.8154... Val Loss: 2.0763\n",
            "Epoch: 13/20... Step: 470... Loss: 1.8109... Val Loss: 2.0642\n",
            "Epoch: 13/20... Step: 480... Loss: 1.8067... Val Loss: 2.0488\n",
            "Epoch: 14/20... Step: 490... Loss: 1.7641... Val Loss: 2.0380\n",
            "Epoch: 14/20... Step: 500... Loss: 1.7934... Val Loss: 2.0356\n",
            "Epoch: 14/20... Step: 510... Loss: 1.7269... Val Loss: 2.0236\n",
            "Epoch: 15/20... Step: 520... Loss: 1.7599... Val Loss: 2.0124\n",
            "Epoch: 15/20... Step: 530... Loss: 1.7179... Val Loss: 2.0059\n",
            "Epoch: 15/20... Step: 540... Loss: 1.7202... Val Loss: 2.0005\n",
            "Epoch: 15/20... Step: 550... Loss: 1.7063... Val Loss: 1.9951\n",
            "Epoch: 16/20... Step: 560... Loss: 1.6861... Val Loss: 1.9961\n",
            "Epoch: 16/20... Step: 570... Loss: 1.6847... Val Loss: 1.9770\n",
            "Epoch: 16/20... Step: 580... Loss: 1.6907... Val Loss: 1.9760\n",
            "Epoch: 16/20... Step: 590... Loss: 1.6769... Val Loss: 1.9590\n",
            "Epoch: 17/20... Step: 600... Loss: 1.6467... Val Loss: 1.9589\n",
            "Epoch: 17/20... Step: 610... Loss: 1.6395... Val Loss: 1.9552\n",
            "Epoch: 17/20... Step: 620... Loss: 1.6378... Val Loss: 1.9459\n",
            "Epoch: 18/20... Step: 630... Loss: 1.6895... Val Loss: 1.9375\n",
            "Epoch: 18/20... Step: 640... Loss: 1.6295... Val Loss: 1.9318\n",
            "Epoch: 18/20... Step: 650... Loss: 1.5898... Val Loss: 1.9299\n",
            "Epoch: 18/20... Step: 660... Loss: 1.5971... Val Loss: 1.9193\n",
            "Epoch: 19/20... Step: 670... Loss: 1.6008... Val Loss: 1.9151\n",
            "Epoch: 19/20... Step: 680... Loss: 1.5973... Val Loss: 1.9144\n",
            "Epoch: 19/20... Step: 690... Loss: 1.5813... Val Loss: 1.9015\n",
            "Epoch: 19/20... Step: 700... Loss: 1.5742... Val Loss: 1.8999\n",
            "Epoch: 20/20... Step: 710... Loss: 1.5296... Val Loss: 1.9017\n",
            "Epoch: 20/20... Step: 720... Loss: 1.5685... Val Loss: 1.9041\n",
            "Epoch: 20/20... Step: 730... Loss: 1.5276... Val Loss: 1.8920\n",
            "Epoch: 20/20... Step: 740... Loss: 1.5586... Val Loss: 1.8884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "vjI25GxZP0as"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "\n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "\n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "metadata": {
        "id": "PWqUknhyP6Ky"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval() # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "tFSKoGamP9mW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime='krishna was saying', top_k=5))\n"
      ],
      "metadata": {
        "id": "KMnYGoXKP_lC",
        "outputId": "d7e1ced3-7a51-4d6b-bd0f-35b6b91fdbcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "krishna was saying alwanss off the sand of men to be\n",
            "deing\n",
            "which alous that are cals that time. Wele the\n",
            "goods of howeres, and a compenter the suct or born and the posels.\"[19] And the moss free from that was heaven\n",
            "the formmors\n",
            "of the pricesting thou has comperied if to the senses of goed\n",
            "by mounter. When the ere this as the sins of all selfous of sire. Those foremost for all creatudes, the son, O monarch, as the elemothar\n",
            "of the\n",
            "gail and all such and the diverse others and say\n",
            "the san of Parsuva, and the ear, which then are seased that this faritity of thy enceed by thy own sound bech offlersing\n",
            "him tood, that there, wos thou ars free for all the sicres of the mention of the mind, and he huver it is the menis of this worlds. The worts, worshind those fells, O paistors, accepting a merince and the hero, with the\n",
            "mone of all\n",
            "the sich son of Patara, thou hast\n",
            "that sensite in that fell for the sine. The sacrufice of\n",
            "his constution of that are seefices. It it subdunce of arowed be the\n",
            "menter of thy came to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "id": "PISkucQ4QBCC",
        "outputId": "87443776-aca0-483c-c51e-9e404db93886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(loaded, 2000, top_k=5, prime=\"karna was\"))"
      ],
      "metadata": {
        "id": "p-L3gec4QDDd",
        "outputId": "b3986f89-97a4-4477-97d8-35532b7abed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "karna was the monertal as\n",
            "the priper of all to be stould be that food which the man along the perpersoul bean armonted and that with a condution and seat and wated\n",
            "on the sive of the great farss). He thousards with many weath, and his beang that from his sacrifice of mine is and the wind. That the foremost of all sensial to the penalos of the son of Dhananjaya,\n",
            "and that fear of the great sanres, thas and that are spach, who ares of\n",
            "men, the\n",
            "his offering the sacrifice, and the sice, and the good of a shall seaked that is as at a tood evan offerend acts was indees, the sacrifices and the son for that soul of the minds and said, -\"O beange and said, 'He weal to\n",
            "han become been\n",
            "to the mounce. Hen having thas indurged wat one, and and sons off is nether,\n",
            "that were offered by the sate of the senfis themefore of all the seases, and thou ald and the\n",
            "senses and thou art the sicce. It induring the seat of\n",
            "heart, and with me that fire, what woss wind that was,. He who are condect with the mind is net took, thenee of his foresoly and\n",
            "with his son of Bhima and was are the son of\n",
            "Pandivas), and that world as\n",
            "her the sons of Brahmanas, the sacrifices\n",
            "if,\n",
            "the sait, whe havongered the soun, and whe than sore are thy suprowed by the sint which\n",
            "his\n",
            "beer all all theme of the geat of the geving\n",
            "and pats of\n",
            "his sair, -In it, O monarch, and the men, O\n",
            "king,. Those frueds of the gais of his body, astroucion and the son of Dhananjaya these words that it. The sun on the posses in the sair to the son of Drithana, and heart, wowh race of thith is thou which heart and a son in highes. And these wind with the mont and high-suces, all he aro this said to me sholy whene the moshing of geat. And who and which he shouldseld by the poseess\n",
            "to hum,\n",
            "the mine. The pratice that tore also all sicrifict, which\n",
            "it is the fore of the\n",
            "mentound, that was to the protuct of his sense of this fall of action, the senses with at the mind, accomplishing have being the migherous\n",
            "ones of the consemont and the\n",
            "seming the\n",
            "son and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlryCmlQS8oj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}